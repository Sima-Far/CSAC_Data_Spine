{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "911a80f2-0c5e-40ba-a858-15c311ad6c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cefbea4a-2e51-4630-a8f1-b2669b8bb629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the required paramethers - time range - list of actors - define output folder and fime name\n",
    "def get_parameters() -> Tuple[str, str, Path, Path, Path, int, bool]:\n",
    "    \"\"\"\n",
    "    Define global parameters.\n",
    "    \"\"\"\n",
    "    start_date = \"1791-01-01\"\n",
    "    end_date = \"2026-01-01\"\n",
    "    actor_file = Path(\"Actors.csv\")\n",
    "    output_dir = Path(\"parquet_output\")\n",
    "    excel_file = Path(\"data_parquet_name_actor_map.xlsx\")\n",
    "    actors_per_batch = 50\n",
    "\n",
    "    # united dataset control\n",
    "    make_united_dataset = False   # <-- TURN ON / OFF HERE I recomend False - It consume too MUCH RAM \n",
    "    united_file = Path(\"actor_day_dataset_united.parquet\")\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return start_date, end_date, actor_file, output_dir, excel_file, actors_per_batch, united_file, make_united_dataset\n",
    "\n",
    "\n",
    "##read actors from file\n",
    "def read_actors_from_csv(actor_file: Path, actor_col: str = \"Actor_Name\") -> List[str]:\n",
    "\n",
    "    \"\"\"\n",
    "    Read actor list from a CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(actor_file)\n",
    "\n",
    "    if actor_col not in df.columns:\n",
    "        raise ValueError(f\"Column '{actor_col}' not found in {actor_file}\")\n",
    "\n",
    "    return df[actor_col].dropna().astype(str).tolist()\n",
    "\n",
    "#generate time range\n",
    "def generate_daily_dates(start_date: str, end_date: str) -> pd.DatetimeIndex:\n",
    "    \"\"\"\n",
    "    Generate a daily date range.\n",
    "    \"\"\"\n",
    "    return pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "\n",
    "# create records - time - day month year - actor\n",
    "def create_actor_day_block(actor: str, dates: pd.DatetimeIndex) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create dataset with one row per actor per day,\n",
    "    including date, day, month, and year columns.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        \"date\": dates,\n",
    "        \"actor\": actor\n",
    "    })\n",
    "\n",
    "    df[\"day\"] = df[\"date\"].dt.day\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "    df[\"year\"] = df[\"date\"].dt.year\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "#generate dataset\n",
    "def build_dataset_parquet():\n",
    "\n",
    "    \"\"\"\n",
    "    Main pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    #read paramethers\n",
    "    start_date, end_date, actor_file, output_dir, excel_file, actors_per_batch, united_file, make_united_dataset = get_parameters()\n",
    "\n",
    "\n",
    "    actors = read_actors_from_csv(actor_file)\n",
    "    dates = generate_daily_dates(start_date, end_date)\n",
    "\n",
    "    buffer = []\n",
    "    batch_id = 1\n",
    "\n",
    "    # mapping list\n",
    "    mapping = []\n",
    "\n",
    "    for i, actor in enumerate(actors, start=1):\n",
    "        df_actor = create_actor_day_block(actor, dates)\n",
    "        buffer.append(df_actor)\n",
    "\n",
    "        # when buffer reaches 5 actors → save\n",
    "        if i % actors_per_batch == 0:\n",
    "            df_out = pd.concat(buffer, ignore_index=True)\n",
    "            file_name = f\"actor_batch_{batch_id:04d}.parquet\"\n",
    "            out_file = output_dir / file_name\n",
    "            df_out.to_parquet(out_file, engine=\"pyarrow\", index=False)\n",
    "            print(f\"Saved {file_name} ({len(df_out):,} rows)\")\n",
    "\n",
    "            # map subdataset name to actors each included (50 actor each)\n",
    "            batch_actors = actors[i-actors_per_batch:i]  # list of 5 actors\n",
    "            mapping.append({\n",
    "                \"parquet_file\": file_name,\n",
    "                \"actors\": \", \".join(batch_actors)\n",
    "            })\n",
    "\n",
    "            buffer.clear()\n",
    "            batch_id += 1\n",
    "\n",
    "    # save remaining actors\n",
    "    if buffer:\n",
    "        df_out = pd.concat(buffer, ignore_index=True)\n",
    "        file_name = f\"actor_batch_{batch_id:04d}.parquet\"\n",
    "        out_file = output_dir / file_name\n",
    "        df_out.to_parquet(out_file, engine=\"pyarrow\", index=False)\n",
    "        print(f\"Saved {file_name} ({len(df_out):,} rows)\")\n",
    "        \n",
    "        # map subdataset name to actors each included (50 actor each)\n",
    "        batch_actors = actors[(batch_id-1)*actors_per_batch:]  # remaining actors\n",
    "        mapping.append({\n",
    "            \"parquet_file\": file_name,\n",
    "            \"actors\": \", \".join(batch_actors)\n",
    "        })\n",
    "\n",
    "    #save mapping Excel\n",
    "    df_map = pd.DataFrame(mapping)\n",
    "    df_map.to_excel(excel_file, index=False)\n",
    "    print(f\"✅ Saved Excel mapping: {excel_file}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # OPTIONAL: build united dataset\n",
    "    # -------------------------\n",
    "    if make_united_dataset:\n",
    "        combine_parquet_to_united_file(output_dir, united_file)\n",
    "        print(\"\\nunited dataset saved\")\n",
    "\n",
    "\n",
    "\n",
    "#read the first and last subdataset to view head and tail of the united dataset    \n",
    "def read_united_parquet_dataset_preview():\n",
    "    _, _, _, output_dir, _, _ ,  _, _= get_parameters()\n",
    "    files = sorted(output_dir.glob(\"*.parquet\"))\n",
    "\n",
    "    print(\"\\nDataset head:\")\n",
    "    print(pd.read_parquet(files[0]).head())\n",
    "\n",
    "    print(\"\\nDataset tail:\")\n",
    "    print(pd.read_parquet(files[-1]).tail())\n",
    "    \n",
    "\n",
    "#Function for merging all sub-dataet and make a united dataset if needed. I do not recommend because it consume tooMUCH RAM   \n",
    "def combine_parquet_to_united_file(output_dir: Path, united_file: Path):\n",
    "    \"\"\"\n",
    "    Read all Parquet files in `output_dir`, combine into one DataFrame,\n",
    "    and save as a single Parquet file (`united_file`).\n",
    "    \"\"\"\n",
    "\n",
    "    # find all parquet files\n",
    "    files = sorted(output_dir.glob(\"*.parquet\"))\n",
    "    # read all files and concatenate\n",
    "    df_list = []\n",
    "    for f in files:\n",
    "        df_list.append(pd.read_parquet(f))\n",
    "    df_united = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # save as one Parquet file\n",
    "    df_united.to_parquet(united_file, engine=\"pyarrow\", index=False)\n",
    "    print(f\"✅ Combined dataset saved to {united_file} ({len(df_united):,} rows)\")\n",
    "\n",
    "    # preview\n",
    "    print(\"\\nDataset head:\")\n",
    "    print(df_united.head())\n",
    "    print(\"\\nDataset tail:\")\n",
    "    print(df_united.tail())\n",
    "\n",
    "    return df_united\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3249dde4-d594-45f1-be6d-6916650e201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved actor_batch_0001.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0002.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0003.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0004.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0005.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0006.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0007.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0008.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0009.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0010.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0011.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0012.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0013.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0014.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0015.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0016.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0017.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0018.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0019.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0020.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0021.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0022.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0023.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0024.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0025.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0026.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0027.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0028.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0029.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0030.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0031.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0032.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0033.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0034.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0035.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0036.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0037.parquet (4,291,650 rows)\n",
      "Saved actor_batch_0038.parquet (171,666 rows)\n",
      "✅ Saved Excel mapping: C:\\Sima\\data_spine\\data_parquet_name_actor_map.xlsx\n",
      "\n",
      "Dataset head:\n",
      "        date                     actor  day  month  year\n",
      "0 1791-01-01  National Liberation Army    1      1  1791\n",
      "1 1791-01-02  National Liberation Army    2      1  1791\n",
      "2 1791-01-03  National Liberation Army    3      1  1791\n",
      "3 1791-01-04  National Liberation Army    4      1  1791\n",
      "4 1791-01-05  National Liberation Army    5      1  1791\n",
      "\n",
      "Dataset tail:\n",
      "             date                         actor  day  month  year\n",
      "171661 2025-12-28  Los Tiguerones Fénix faction   28     12  2025\n",
      "171662 2025-12-29  Los Tiguerones Fénix faction   29     12  2025\n",
      "171663 2025-12-30  Los Tiguerones Fénix faction   30     12  2025\n",
      "171664 2025-12-31  Los Tiguerones Fénix faction   31     12  2025\n",
      "171665 2026-01-01  Los Tiguerones Fénix faction    1      1  2026\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Generate ddatasets\n",
    "    build_dataset_parquet()\n",
    "\n",
    "    #read the first and last datsets to show the preview of head and tail of the united dataset\n",
    "    read_united_parquet_dataset_preview()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177efa0-236d-47aa-a7ec-5b1cd36dcc2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
